{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit=True,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bos_token\": \"<s>\",\n",
      "  \"eos_token\": \"</s>\",\n",
      "  \"unk_token\": \"<unk>\"\n",
      "}\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "print(json.dumps(tokenizer.special_tokens_map, indent=2))\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bos_token\": \"<s>\",\n",
      "  \"eos_token\": \"</s>\",\n",
      "  \"unk_token\": \"<unk>\"\n",
      "}\n",
      "32000\n",
      "</s>\n",
      "<s>\n",
      "<unk>\n",
      "No pad token\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    padding_side=\"left\",\n",
    "    use_fast=False,\n",
    ")\n",
    "print(json.dumps(tokenizer.special_tokens_map, indent=2))\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.eos_token or \"No eos token\")\n",
    "print(tokenizer.bos_token or \"No bos token\")\n",
    "print(tokenizer.unk_token or \"No unk token\")\n",
    "print(tokenizer.pad_token or \"No pad token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_new_tokens = tokenizer.add_special_tokens({\n",
    "#     \"pad_token\": \"<pad>\",\n",
    "#     \"bos_token\": \"<s>\",\n",
    "#     \"eos_token\": \"</s>\",\n",
    "#     \"unk_token\": \"<unk>\",\n",
    "# })\n",
    "# print(tokenizer.all_special_ids)\n",
    "# print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,     0,     0,     0,     0,     1, 22172,  3186],\n",
      "        [    1, 22172,  3186,   306,   626,  2675,   304,  3762]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    [\"hello world\", \"hello world I am going to school\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><s> hello world', '<s> hello world I am going to school']\n"
     ]
    }
   ],
   "source": [
    "decoded_input_ids = tokenizer.batch_decode(input_ids[\"input_ids\"],)\n",
    "print(decoded_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"../cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"b-mc2/sql-create-context\",\n",
    "    cache_dir=\"../cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'CREATE TABLE head (age INTEGER)',\n",
       " 'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'answer': 'SELECT COUNT(*) FROM head WHERE age > 56'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(question, context, answer=\"\"):\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{answer}\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < 512\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt_sql(\n",
    "        data_point[\"question\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"answer\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3a7a1b41c544bca46d333647c8e51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess train data set (num_proc=16):   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = dataset[\"train\"].map(\n",
    "    generate_and_tokenize_prompt,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=next(iter(dataset.values())).column_names,\n",
    "    desc=\"preprocess train data set\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     1,   887,   526,\n",
       "           263, 13988,  1426, 29899,   517, 29899,  4176,  1904, 29889,  3575,\n",
       "          4982,   338,   304,  1234,  5155,  1048,   263,  2566, 29889,   887,\n",
       "           526,  2183,   263,  1139,   322,  3030, 11211,   697,   470,   901,\n",
       "          6131, 29889, 29871,    13,    13,  3492,  1818,  1962,   278,  3758,\n",
       "          2346,   393,  6089,   278,  1139, 29889,    13,    13,  2277, 29937,\n",
       "           894, 29901,    13,  5328,  1784, 15883,   310,   278,  5840,  1860,\n",
       "           526,  9642,  1135, 29871, 29945, 29953,  1577,    13,    13,  2277,\n",
       "         29937, 15228, 29901,    13, 27045, 10911,  2343,   313,   482,  2672,\n",
       "          4330, 17070, 29897,    13,    13,  2277, 29937, 13291, 29901,    13,\n",
       "          6404, 21122, 22798,  3895,  2343,  5754,  5046,  1405, 29871, 29945,\n",
       "         29953,     2]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,     1,   887,   526,\n",
       "           263, 13988,  1426, 29899,   517, 29899,  4176,  1904, 29889,  3575,\n",
       "          4982,   338,   304,  1234,  5155,  1048,   263,  2566, 29889,   887,\n",
       "           526,  2183,   263,  1139,   322,  3030, 11211,   697,   470,   901,\n",
       "          6131, 29889, 29871,    13,    13,  3492,  1818,  1962,   278,  3758,\n",
       "          2346,   393,  6089,   278,  1139, 29889,    13,    13,  2277, 29937,\n",
       "           894, 29901,    13,  5328,  1784, 15883,   310,   278,  5840,  1860,\n",
       "           526,  9642,  1135, 29871, 29945, 29953,  1577,    13,    13,  2277,\n",
       "         29937, 15228, 29901,    13, 27045, 10911,  2343,   313,   482,  2672,\n",
       "          4330, 17070, 29897,    13,    13,  2277, 29937, 13291, 29901,    13,\n",
       "          6404, 21122, 22798,  3895,  2343,  5754,  5046,  1405, 29871, 29945,\n",
       "         29953,     2]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator([train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
