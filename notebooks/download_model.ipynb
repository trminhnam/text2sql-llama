{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/7B/llama-7b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2180.23 MB (+  256.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 16 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 16/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 2026 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/7B/llama-7b.ggmlv3.q4_0.bin\"\n",
    "llm = Llama(\n",
    "    model_path=model_path, \n",
    "    n_gpu_layers=16,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"cmpl-8b964a35-cf24-4aba-b912-86bb7d55f02b\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1692889899,\n",
      "    \"model\": \"../models/7B/llama-7b.ggmlv3.q4_0.bin\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"text\": \" Bạn học được.\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 18,\n",
      "        \"completion_tokens\": 12,\n",
      "        \"total_tokens\": 30\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  9720.98 ms\n",
      "llama_print_timings:      sample time =     6.93 ms /    13 runs   (    0.53 ms per token,  1876.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9720.91 ms /    18 tokens (  540.05 ms per token,     1.85 tokens per second)\n",
      "llama_print_timings:        eval time =  1273.86 ms /    12 runs   (  106.16 ms per token,     9.42 tokens per second)\n",
      "llama_print_timings:       total time = 11030.32 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    \"Translate the following sentence to Vietnamese: I go to school. Vietnamese:\", \n",
    "    max_tokens=1024,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=False,\n",
    ")\n",
    "print(json.dumps(output, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-c066edea-3c53-4075-b204-d2b2d6d9a291',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1692889910,\n",
       " 'model': '../models/7B/llama-7b.ggmlv3.q4_0.bin',\n",
       " 'choices': [{'text': '\\n\\\\strong{Possible Duplicate:}\\n\\n[Stephen Colbert vs. John Oliver: who would win?](https://english.stackexchange.com/questions/67284/stephen-colbert-vs-john-oliver-who-would-win)\\n\\nWhat I am asking is not a direct comparison between Stephen and John, but more of a debate where the two go back and forth with their best comedic insults.\\n\\nI think Colbert has a way with words, he can be very creative in his comedic insults/put down',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 16, 'completion_tokens': 128, 'total_tokens': 144}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  9720.98 ms\n",
      "llama_print_timings:      sample time =    71.48 ms /   128 runs   (    0.56 ms per token,  1790.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1201.20 ms /    15 tokens (   80.08 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:        eval time = 12883.65 ms /   127 runs   (  101.45 ms per token,     9.86 tokens per second)\n",
      "llama_print_timings:       total time = 14432.82 ms\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
