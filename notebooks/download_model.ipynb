{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from models/7B/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3615.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10a531a60\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10a531cc0\n",
      "ggml_metal_init: loaded kernel_mul                            0x10a4fc4c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10a4fcdc0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10a4fd6c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10a4fe140\n",
      "ggml_metal_init: loaded kernel_relu                           0x10a4febc0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10a4ff390\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10a4ff5f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10a4ffa70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10a4ffcd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10df04080\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10df042e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10df04540\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10a531f20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10df047a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10df04a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10df04c60\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10df04ec0\n",
      "ggml_metal_init: loaded kernel_norm                           0x10df05120\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10df05380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10df055e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10df05840\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10df05aa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10c133a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10c133cc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10c133f20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10c134180\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x10c1343e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x10c134640\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x10c1348a0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x10c134b00\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x10c134d60\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x10c134fc0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x10c135220\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x10c135480\n",
      "ggml_metal_init: loaded kernel_rope                           0x10c1356e0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10c135940\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10c135ba0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10c135e00\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10c136060\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.35 MB\n",
      "llama_new_context_with_model: max tensor size =    70.31 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.08 MB, ( 7569.00 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, ( 7570.36 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 7828.36 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, ( 7900.38 / 10922.67)\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/7B/llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "llm = Llama(\n",
    "    model_path=model_path, \n",
    "    n_gpu_layers=1, \n",
    "    low_vram=True, \n",
    "    verbose=False,\n",
    "    logits_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"cmpl-873c1717-2e91-4a38-a0cd-ed6ce972b3b5\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1692621157,\n",
      "    \"model\": \"models/7B/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"text\": \" Tôi học trường.\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 18,\n",
      "        \"completion_tokens\": 11,\n",
      "        \"total_tokens\": 29\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    \"Translate the following sentence to Vietnamese: I go to school. Vietnamese:\", \n",
    "    max_tokens=1024,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=False,\n",
    ")\n",
    "print(json.dumps(output, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-43ecf9df-f470-45c9-be21-4a354d095746',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1692621163,\n",
       " 'model': 'models/7B/llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
       " 'choices': [{'text': \"\\nStephen Colbert: Yo, John, I hear you've been talkin' smack about me on your show.\\n\\nJohn Oliver: Oh, yeah? What have I been sayin', Stephen?\\n\\nStephen Colbert: You said I'm not funny and my jokes are stale. That I should stick to impressions of people I don't know.\\n\\nJohn Oliver: (smirking) Oh, that's right! And you know what? You're right! Your impressions are the only thing good about your show. The rest is just a\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 16, 'completion_tokens': 128, 'total_tokens': 144}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
